{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CNN Histopathologic Cancer Detection Mini‑Project**\n",
    "\n",
    "This notebook follows the Week 3 mini‑project, the [Kaggle Histopathologic Cancer Detection competition](https://www.kaggle.com/c/histopathologic-cancer-detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem Description & Data Overview**\n",
    "\n",
    "The goal is to accurately detect metastatic cancer in histopathologic images of lymph node sections. This classification task plays a critical role in cancer diagnostics and patient prognosis. This classification task plays a critical role in cancer diagnostics and patient prognosis.\n",
    "\n",
    "Improving accuracy and automation in cancer detection reduces the burden on pathologists and helps with faster, more consistent diagnoses. The goal build a CNN model using EfficientNetB0 with transfer learning to classify image tiles as cancerous or not.\n",
    "\n",
    "\n",
    "We tackle a **binary image‑classification** task: detect metastatic cancer in 32 × 32‑pixel histopathology image patches.\n",
    "\n",
    "| Item | Details |\n",
    "|------|---------|\n",
    "| **Input** | RGB `.tif` images (`train/` & `test/` folders) |\n",
    "| **Labels** | `train_labels.csv` (`id`, `label` — 1 = cancer, 0 = benign) |\n",
    "| **Metric** | Area Under the ROC Curve (AUC) |\n",
    "| **Goal** | Build a CNN that achieves reasonable AUC on Kaggle and document the full ML workflow.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T22:20:14.787778Z",
     "iopub.status.busy": "2025-04-22T22:20:14.787368Z",
     "iopub.status.idle": "2025-04-22T22:20:33.488403Z",
     "shell.execute_reply": "2025-04-22T22:20:33.487387Z",
     "shell.execute_reply.started": "2025-04-22T22:20:14.787748Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, glob, random, math, json, warnings, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/histopathologic-cancer-detection\"\n",
    "TRAIN_IMG_DIR = f\"{DATA_DIR}/train\"\n",
    "TEST_IMG_DIR  = f\"{DATA_DIR}/test\"\n",
    "LABELS_CSV    = f\"{DATA_DIR}/train_labels.csv\"\n",
    "\n",
    "assert os.path.exists(TRAIN_IMG_DIR), \"Check DATA_DIR paths!\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs detected:\", gpus)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"⚠️  No GPU found — training will run on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "We analyze the dataset's structure, including image samples and class distribution.\n",
    "The class distribution is imbalanced, with a higher number of non-cancerous tiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "print(labels_df.head())\n",
    "print(labels_df.label.value_counts())\n",
    "\n",
    "# Class distribution plot\n",
    "sns.countplot(data=labels_df, x='label')\n",
    "plt.title('Class distribution (0 = benign, 1 = cancer)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NOTE**\n",
    "The imbalance suggests that class weights or data augmentation may be helpful. Visualization confirms that cancerous regions are harder to visually distinguish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# show 6 sample images per class\n",
    "def show_samples(df, n=6, cancer=0):\n",
    "    ids = df[df.label==cancer].sample(n)['id'].values\n",
    "    plt.figure(figsize=(12,2))\n",
    "    for i, img_id in enumerate(ids):\n",
    "        img_path = os.path.join(TRAIN_IMG_DIR, f\"{img_id}.tif\")\n",
    "        img = tf.keras.utils.load_img(img_path)\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'label={cancer}')\n",
    "    plt.show()\n",
    "\n",
    "show_samples(labels_df, n=6, cancer=0)\n",
    "show_samples(labels_df, n=6, cancer=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation**\n",
    "\n",
    "When preparing the data, there were a few things I needed to take note of:\n",
    "\n",
    "- Image decoding using PIL within a TensorFlow pipeline.\n",
    "- Resize all images to 96x96.\n",
    "- Normalize pixel values.\n",
    "- Augment training data with flips, zooms, and rotations.\n",
    "- Split the dataset into training and validation sets.\n",
    "\n",
    "I even left some of those variables aside to be able to tamper with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE   = 96\n",
    "BATCH_SIZE = 64\n",
    "AUTO       = tf.data.AUTOTUNE\n",
    "\n",
    "def _pil_load_resize(path):\n",
    "    path = path.numpy().decode(\"utf-8\")     # EagerTensor → bytes → str\n",
    "    img  = Image.open(path)\n",
    "    img  = img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
    "    return np.asarray(img, np.float32) / 255.0\n",
    "\n",
    "def decode_image(filename, label=None):\n",
    "    img = tf.py_function(_pil_load_resize, [filename], Tout=tf.float32)\n",
    "    img.set_shape([IMG_SIZE, IMG_SIZE, 3])  # static shape for TF\n",
    "    return (img, label) if label is not None else img\n",
    "\n",
    "filepaths = [os.path.join(TRAIN_IMG_DIR, f\"{i}.tif\") for i in labels_df.id]\n",
    "labels    = labels_df.label.values\n",
    "\n",
    "ds_full = (\n",
    "    tf.data.Dataset\n",
    "      .from_tensor_slices((filepaths, labels))\n",
    "      .shuffle(2048, seed=42)\n",
    "      .map(decode_image, num_parallel_calls=AUTO)\n",
    ")\n",
    "\n",
    "val_size = int(len(labels) * 0.2)\n",
    "val_ds   = ds_full.take(val_size).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "train_ds = ds_full.skip(val_size).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "print(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Baseline CNN Architecture**\n",
    "\n",
    "I deemed the following model and configurations to be efficient enough to get a good accuracy:\n",
    "\n",
    "EfficientNetB0 pretrained on ImageNet as the base. A dense head with 128 units and dropout is used before the final sigmoid output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation='relu'),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n",
    "\n",
    "Let's get into some training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-22T22:17:52.195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint('best_cnn.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the EfficientNet base is initially frozen to allow the newly added dense layers to adapt without disrupting pretrained weights. The model is trained using binary crossentropy as the loss function and AUC as a performance metric. \n",
    "\n",
    "To avoid overfitting, early stopping is used alongside model checkpointing to save the best-performing model. Class weights are computed to mitigate the effect of class imbalance present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-22T22:17:52.195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Plot training curves\n",
    "def plot_history(hist):\n",
    "    for metric in ['accuracy', 'loss', 'auc']:\n",
    "        plt.figure()\n",
    "        plt.plot(hist.history[metric], label=f'train_{metric}')\n",
    "        plt.plot(hist.history[f'val_{metric}'], label=f'val_{metric}')\n",
    "        plt.title(metric)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation & Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-22T22:17:52.195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_preds = model.predict(val_ds).ravel()\n",
    "val_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "val_auc = roc_auc_score(val_labels, val_preds)\n",
    "print(f'Validation AUC: {val_auc:.4f}')\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(val_labels, val_preds)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC={val_auc:.3f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved a high AUC score on the validation set, indicating strong discriminative ability between cancerous and non-cancerous image tiles. The ROC curve demonstrates that the model performs well across different classification thresholds. Additional insights can be gathered through the confusion matrix and classification report, which provide a more detailed performance breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generate Test Predictions & Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-22T22:17:52.195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create tf.data for test images\n",
    "test_files = sorted(glob.glob(os.path.join(TEST_IMG_DIR, '*.tif')))\n",
    "test_ids   = [os.path.basename(x).split('.')[0] for x in test_files]\n",
    "\n",
    "test_ds = (tf.data.Dataset.from_tensor_slices(test_files)\n",
    "           .map(lambda x: decode_image(x), num_parallel_calls=AUTO)\n",
    "           .batch(BATCH_SIZE))\n",
    "\n",
    "test_preds = model.predict(test_ds).ravel()\n",
    "\n",
    "sub_df = pd.DataFrame({'id': test_ids, 'label': test_preds})\n",
    "sub_path = 'submission.csv'\n",
    "sub_df.to_csv(sub_path, index=False)\n",
    "print('Created submission:', sub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "Transfer learning significantly boosts performance on limited data. Image preprocessing and augmentation are crucial steps in building a robust model. The final model generalizes well to unseen validation and test data.\n",
    "\n",
    "\n",
    "#### What are next steps?\n",
    "\n",
    "The next steps can include:\n",
    "- Unfreezing the base layers and fine-tuning the model\n",
    "- Experimenting with larger EfficientNet variants or other backbone architectures\n",
    "- Applying ensembling methods to improve prediction robustness\n",
    "\n",
    "Additionally, it would be beneficial to investigate misclassified images for potential data labeling errors or opportunities to improve augmentation techniques.\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 862157,
     "sourceId": 11848,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
